<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Designing a mini Roomba | Francisco Zenteno Smith</title> <meta name="author" content="Francisco Zenteno Smith"> <meta name="description" content="Perception, planning and controls for mobile robotics in 2D"> <meta name="keywords" content="Operations Research, Optimization, Simulation"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://fjzs.github.io/projects/05_robotics/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Francisco </span>Zenteno Smith</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Designing a mini Roomba</h1> <p class="post-description">Perception, planning and controls for mobile robotics in 2D</p> </header> <article> <p>This project is part of the course “Introduction to Robotics” taught by professor Henrik Christensen in the Computer Science Department at UC San Diego. Link of the course is here: <a href="http://www.hichristensen.net/CSE276A-23/index.html" rel="external nofollow noopener" target="_blank">http://www.hichristensen.net/CSE276A-23/index.html</a></p> <p>Project repository: <a href="https://github.com/fjzs/mobile-robotics-2d" rel="external nofollow noopener" target="_blank">https://github.com/fjzs/mobile-robotics-2d</a></p> <h2 id="1-problem-definition"><strong>1 Problem Definition</strong></h2> <p>Given a Four Mecanum Wheeled Mobile Robot, design a “Roomba”-like system for a 2 x 2 square meters zone (free of obstacles). Use 12 April Tags to localize the robot as shown in Figure 1. Provide an algorithm to clean the zone, a video of it and a performance guarantee for this coverage.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/area_description.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 1: the area to cover is a square 2x2 square meters surrounded by april tags to help localize the robot.</figcaption> </figure> </div></div> <h4 id="11-what-is-a-four-mecanum-wheeled-mobile-robot"><strong>1.1 What is a Four Mecanum Wheeled Mobile Robot?</strong></h4> <p>A mecanum wheel is an omnidirectional wheel design, providing the capability to move in 2D and rotate along the z axis, as opposed to traditional wheels that can only move in 1D. In this project we followed and implemented the kinematic model provided by Taheri et al (2015) (<a href="https://research.ijcaonline.org/volume113/number3/pxc3901586.pdf" rel="external nofollow noopener" target="_blank">https://research.ijcaonline.org/volume113/number3/pxc3901586.pdf</a>). In Figure 2 you can see the kinematic configuration of this robot. Each wheel \(i\) has an angular velocity of \(\omega_i\), providing the robot with a 2D velocity \((v_x, v_y)\) and a rotation around the z axis (yaw) given by \(\omega_z\).</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/mecanum_kinematic.png" class="img-fluid rounded z-depth-1" width="800" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 2: On the left you can see the kinematic configuration of this robot. Notice that the robot coordinate frame is located at the center of the robot (O), and Xr axis is towards the front of the robot, and Yr is towards the left of the robot. Source: Taheri et al (2015)</figcaption> </figure> </div></div> <h4 id="12-the-particular-robot-used"><strong>1.2 The particular robot used</strong></h4> <p>In this project the robot consisted in a Mbot Mega (4 mecanum wheel robot) + Qualcomm RB5 board. This robot is equipped with a monocular camera that we will use for perceptions tasks. You can see it in Figure 3.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/robot_rb5.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 3: this is the robot we used, which provides enough computing resources and a camera to perform multiple robotic tasks.</figcaption> </figure> </div></div> <h4 id="13-what-is-an-april-tag-and-how-to-use-it-for-localization"><strong>1.3 What is an April Tag and how to use it for localization</strong></h4> <p>April Tag is a visual system developed at University of Michigan (<a href="https://april.eecs.umich.edu/software/apriltag" rel="external nofollow noopener" target="_blank">https://april.eecs.umich.edu/software/apriltag</a>). It’s like a QR code that can be read by a library developed by the authors. In Figure 4 there is a description of how this works. Given the coordinate frames of the world (w), robot (r), robot’s camera (c) and april tag (a), when the robot’s camera reads an april tag, the library retrieves both the id of the tag and the 4x4 transformation \(^{c}T_{a}\), which is the relative transformation that takes a point in the april tag coordinate frame to the robot’s camera coordinate frame. Then, using that \(^{c}T_{a} = (^{a}T_{c})^{-1}\), we can get the robot position (x,y,z) and orientation (roll, pitch, yaw) in the world frame using the following equation:</p> \[^{w}T_{r} = ^{w}T_{a} \cdot ^{a}T_{c} \cdot ^{c}T_{r}\] <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/april_tag.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 4: An april tag is like a QR code that when read by the April Tag library, returns the id of the tag and the relative position and orientation of the camera with respect to the april tag. In this diagram we have 4 coordinate frames: the world frame (w), the robot frame (r), the robot's camera frame (c) and the april tag frame (a). Here we show the april tag id=0 for family 36h11 specifically, but there are multiple families and ids of april tags.</figcaption> </figure> </div></div> <h2 id="2-solution"><strong>2 Solution</strong></h2> <p>We used ROS + python as the main framework to develop this project. The environment of this project is shown in Figure 5.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/environment.png" class="img-fluid rounded z-depth-1" width="700" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 5: This is a 2x2 meters zone that the robot must cover. We have 12 april tags at fixed positions and orientations that will provide localization to the robot.</figcaption> </figure> </div></div> <p>The architecture of our system is shown in Figure 6. In the next subsections we explain the responsibilities of each module.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/architecture.png" class="img-fluid rounded z-depth-1" width="700" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 6: Architecture of the system</figcaption> </figure> </div></div> <h4 id="21-initialization"><strong>2.1 Initialization</strong></h4> <p>This module is responsible for configuring the environment and the robot in the system, such as:</p> <ul> <li>The size of the environment</li> <li>The coordinate frame of the world frame</li> <li>The transformations matrices (orientation and position) of each april tag in world frame</li> <li>The initial position of the robot</li> <li>The transformations matrices of the robot’s camera with respect to the robot</li> <li>The time interval of the loop (for instance, 0.05 seconds)</li> <li>The parameters of the kinematic model of the robot</li> </ul> <h4 id="22-path-planning"><strong>2.2 Path Planning</strong></h4> <p>This module is responsible for providing a set of waypoints \((x, y, \theta)\) for the robot to follow in order to cover the area. To do this, first a world representation is created, which in this case corresponds to a 2D grid. The size of each cell is a parameter of the module. In this project we chose to use 0.2 [m] as the cell side length, because the robot length is roughly 0.2 [m] as well. Then, given this grid, another algorithm decides how to cover it. In this case we chose a mine sweeper style to cover the grid (graph), which is shown in Figure 7.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/path_planning.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 7: the robot starts in the green start and goes through the waypoints (red Xs) all the way down to the blue circle. We decided to spread the waypoints in this fashion because of the noise of the sensors.</figcaption> </figure> </div></div> <h4 id="23-perception"><strong>2.3 Perception</strong></h4> <p>The only sensor of this robot is the camera, which is used to detect April Tags. Then, as shown in section 1.3, we can use the April Tag library to estimate the robot state given by its position and orientation \((x,y,\theta)\). In this environment, nonetheless, the robot is going to detect multiple April Tags, all of them in different ranges and with different magnitudes of noise (given by illumination, motion, blurriness, etc). To handle the noise and to provide the best estimation of the robot state we used the median of the April Tag detections, which reduced the noise, but to a point where it was manageable. We also measured the noise of this approach by sampling the localization error on different robot positions, and found out that the localization error distributes ~ \(N(0, 0.05)\) as shown in Figure 8.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/gaussian_fit.png" class="img-fluid rounded z-depth-1" width="600" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 8: With the error measurements we took we were able to fit a Gaussian distribution of the localization error. With roughly 1500 points, we estimated that μ = 0.00 and σ = 0.05. The green bars represent the empirical density and the dashed line the theoretical density.</figcaption> </figure> </div></div> <h4 id="24-short-term-plan"><strong>2.4 Short-term Plan</strong></h4> <p>This module is responsible for deciding at every time step \(t\) the velocity of the robot \((v_x, v_y, \omega_z)\) given the current robot state \(s_t\) and the next waypoint \(w\) to reach. To get this velocity we used a PID controller, with minimum and maximum values for the norm of the velocity vector.</p> <p>Then, given this velocity \((v_x, v_y, \omega_z)\), this module also translates this into angular velocities \(\omega_i\) for each wheel \(i\) given the kinematic model of the robot. Figure 9 shows the kinematic model of this type of robot.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/forward_kinematic_model.png" class="img-fluid rounded z-depth-1" width="300" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 9: Forward Kinematic Model: this matrix translates a required velocity in 2D and a rotation around Z axis (omega z) into angular velocities at each wheel of the robot. lx is half the distance between the front wheels, ly is half the distance between the fron and rear wheels, and r is the radius of the wheel. Source: Taheri et al (2015)</figcaption> </figure> </div></div> <h4 id="25-control"><strong>2.5 Control</strong></h4> <p>This module is responsible for translating the angular velocities \((\omega_1, \omega_2, \omega_3, \omega_4)\) required by the Short-term Planning module into motor commands to achieve those velocities. Sometimes this task is also called “calibration”. To do this, I sampled multiple straight trajectories of the robot given different motor commands (which I refer to “power”, they could be anywhere from -5000 to 5000). With this experiment I could get a function relating power to the wheel to angular velocity of the wheel, as shown in Figure 10.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/calibration.png" class="img-fluid rounded z-depth-1" width="600" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 10: This graph shows the relation between power (the motor command interface provided by the robot) and the actual angular velocity achieved. Two things to notice: first, this calibration only works for the surface it was measured because of different friction coefficients, and second, notice that below a power input of 40 the robot won't move because of this friction.</figcaption> </figure> </div></div> <h2 id="3-results"><strong>3 Results</strong></h2> <h4 id="31-robot-trajectory"><strong>3.1 Robot trajectory</strong></h4> <p>In Figure 11 we show the trajectory estimated by the robot perception stack throught its journey. The <strong>video</strong> associated to this experiment can be seen in this link: <a href="https://youtu.be/yTnWUw7eKxI" rel="external nofollow noopener" target="_blank">https://youtu.be/yTnWUw7eKxI</a>.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/trajectory_estimation.png" class="img-fluid rounded z-depth-1" width="600" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 11: This plot shows the beginning of the trajectory (estimated by the robot) in the green star up to the point near (0,0). The system is very noisy because of the quality of the sensors is not good, nonetheless, the robot managed to cover the area in a reasonable time.</figcaption> </figure> </div></div> <h4 id="32-performance-guarantee"><strong>3.2 Performance guarantee</strong></h4> <p>In this section we explain a simplified approach to provide a performance guarantee considering only a single dimension. The question we are trying to answer here is: what is the expected coverage percentage of our robot in a 1D grid cell, considering that the localization error follows a probability distribution and that the robot is considered to cover that cell when it is at a minimum threshold distance? Figure 12 shows our method.</p> <div class="row"><div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/robotics/coverage_diagram.png" class="img-fluid rounded z-depth-1" width="800" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture><figcaption class="caption">Figure 12: his diagram shows our simplified approach for estimating the stochastic coverage of a 1D cell. If there was no error noise and a threshold τ = 0, the robot estimate would be the same as the center of the cell (star icon), thus achieving a 100% coverage. But, because of noise, we allow the robot to mark the cell as covered when being within a threshold τ of its center, so in the best case the robot is going to be τ from the real center of the cell. On top of this, we know the localization error follows a probability distribution, so can sample a value from this distribution to get a stochastic estimate of the real position of the robot. That would give us the real coverage of the robot given by the intersection of the robot interval and the cell interval, which is the red interval.</figcaption> </figure> </div></div> <p>By using Monte Carlo simulation on this stochastic system we can expect on average, that each cell of the grid is going to be covered roughly by 47% on the first pass of the roomba.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Francisco Zenteno Smith. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0R41J5CTCW"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0R41J5CTCW");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>